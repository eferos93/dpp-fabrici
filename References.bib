@article{recordLinkTheory,
author = { Ivan P.   Fellegi  and  Alan B.   Sunter },
title = {A Theory for Record Linkage},
journal = {Journal of the American Statistical Association},
volume = {64},
number = {328},
pages = {1183-1210},
year  = {1969},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1969.10501049},
}

@inproceedings{dataModelIntegration1980,
author = {El-Masri, Ramez and Wiederhold, Gio},
title = {Data Model Integration Using the Structural Model},
year = {1979},
isbn = {089791001X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/582095.582127},
doi = {10.1145/582095.582127},
abstract = {One approach to the design of a logical model for an integrated database requires each potential user or application to specify its view as a data model. An integration phase follows, where these user data models are integrated into a global database model. We address the problem of view integration when user data models are expressed using the structural model [Wi77, WE79].The structural model is built from relations in Boyce-Codd normal form [Co74]. A basic set of integrity assertions is implicit in the model. The integrity assertions are defined by classification of relations into types, and are represented by connections between relations. We will show how to integrate different representations of two related real-world entity classes.},
booktitle = {Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data},
pages = {191–202},
numpages = {12},
keywords = {data semantics, data model integration, logical database design, conceptual and external schema, structural model, entity classes and relationships, ansi/sparc dbms architecture, relational model},
location = {Boston, Massachusetts},
series = {SIGMOD '79}
}

@article{Dong2013,
abstract = {The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community. {\textcopyright} 2013 IEEE.},
author = {Dong, Xin Luna and Srivastava, Divesh},
doi = {10.1109/ICDE.2013.6544914},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Srivastava - 2013 - Big data integration.pdf:pdf},
isbn = {9781467349086},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
keywords = {Publisher: Morgan & Claypool Publishers, Composito},
number = {September},
pages = {1245--1248},
title = {{Big data integration}},
year = {2013}
}

@article{yu2016big,
  title={Big privacy: Challenges and opportunities of privacy study in the age of big data},
  author={Yu, Shui},
  journal={IEEE access},
  volume={4},
  pages={2751--2763},
  year={2016},
  publisher={IEEE}
}

@article{Cormode2011,
abstract = {Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance.We also discuss the tradeoffs between the different synopsis types. {\textcopyright} 2012 G. Cormode, M. Garofalakis, P. J. Haas and C. Jermaine.},
author = {Cormode, Graham and Garofalakis, Minos and Haas, Peter J. and Jermaine, Chris},
doi = {10.1561/1900000004},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cormode et al. - 2011 - Synopses for massive data Samples, histograms, wavelets, sketches.pdf:pdf},
issn = {19317883},
journal = {Foundations and Trends in Databases},
number = {1-3},
pages = {1--294},
title = {{Synopses for massive data: Samples, histograms, wavelets, sketches}},
volume = {4},
year = {2011}
}


@article{Gribonval2020,
abstract = {This article considers "compressive learning," an approach to large-scale machine learning where datasets are massively compressed before learning (e.g., clustering, classification, or regression) is performed. In particular, a "sketch" is first constructed by computing carefully chosen nonlinear random features (e.g., random Fourier features) and averaging them over the whole dataset. Parameters are then learned from the sketch, without access to the original dataset. This article surveys the current state-of-the-art in compressive learning, including the main concepts and algorithms, their connections with established signal-processing methods, existing theoretical guarantees -- on both information preservation and privacy preservation, and important open problems.},
archivePrefix = {arXiv},
arxivId = {2008.01839},
author = {Gribonval, R{\'{e}}mi and Chatalic, Antoine and Keriven, Nicolas and Schellekens, Vincent and Jacques, Laurent and Schniter, Philip},
eprint = {2008.01839},
file = {:home/eferos93/Downloads/2008.01839.pdf:pdf},
pages = {1--35},
title = {{Sketching Datasets for Large-Scale Learning (long version)}},
url = {http://arxiv.org/abs/2008.01839},
year = {2020}
}

@article{Antonanzas2021,
abstract = {Time series data can be subject to changes in the underlying process that generates them and, because of these changes, models built on old samples can become obsolete or perform poorly. In this work, we present a way to incorporate information about the current data distribution and its evolution across time into machine learning algorithms. Our solution is based on efficiently maintaining statistics, particularly the mean and the variance, of data features at different time resolutions. These data summarisations can be performed over the input attributes, in which case they can then be fed into the model as additional input features, or over latent representations learned by models, such as those of Recurrent Neural Networks. In classification tasks, the proposed techniques can significantly outperform the prediction capabilities of equivalent architectures with no feature / latent summarisations. Furthermore, these modifications do not introduce notable computational and memory overhead when properly adjusted.},
archivePrefix = {arXiv},
arxivId = {2108.11923},
author = {Antonanzas, Jesus and Arias, Marta and Bifet, Albert},
eprint = {2108.11923},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antonanzas, Arias, Bifet - 2021 - Sketches for Time-Dependent Machine Learning(2).pdf:pdf},
keywords = {machine learning,recurrent neural networks,stream processing,time series analysis},
number = {1},
pages = {1--9},
title = {{Sketches for Time-Dependent Machine Learning}},
url = {http://arxiv.org/abs/2108.11923},
year = {2021}
}


@article{Jiang2018,
abstract = {To address the challenge of explosive big data, distributed machine learning (ML) has drawn the interests of many researchers. Since many distributed ML algorithms trained by stochastic gradient descent (SGD) involve communicating gradients through the network, it is important to compress the transferred gradient. A category of low-precision algorithms can significantly reduce the size of gradients, at the expense of some precision loss. However, existing low-precision methods are not suitable for many cases where the gradients are sparse and nonuniformly distributed. In this paper, we study is there a compression method that can efficiently handle a sparse and nonuniform gradient consisting of key-value pairs? Our first contribution is a sketch based method that compresses the gradient values. Sketch is a class of algorithms using a probabilistic data structure to approximate the distribution of input data. We design a quantile-bucket quantification method that uses a quantile sketch to sort gradient values into buckets and encodes them with the bucket indexes. To further compress the bucket indexes, our second contribution is a sketch algorithm, namely MinMaxSketch. MinMaxSketch builds a set of hash tables and solves hash collisions with a MinMax strategy. The third contribution of this paper is a delta-binary encoding method that calculates the increment of the gradient keys and stores them with fewer bytes. We also theoretically discuss the correctness and the error bound of three proposed methods. To the best of our knowledge, this is the first effort combining data sketch with ML. We implement a prototype system in a real cluster of our industrial partner Tencent Inc., and show that our method is up to 10× faster than existing methods.},
author = {Jiang, Jiawei and Fu, Fangcheng and Yang, Tong and Cui, Bin},
doi = {10.1145/3183713.3196894},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2018 - SketchML Accelerating distributed machine learning with data sketches.pdf:pdf},
isbn = {9781450317436},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Distributed machine learning,Frequency sketch,Quantification,Quantile sketch,Stochastic gradient Descent},
pages = {1269--1284},
title = {{SketchML: Accelerating distributed machine learning with data sketches}},
year = {2018}
}

@article{Dwork2013,
abstract = {The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition. After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations - not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed. We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed. Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey- there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it. {\textcopyright} 2014 C. Dwork and A. Roth.},
author = {Dwork, Cynthia and Roth, Aaron},
doi = {10.1561/0400000042},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork, Roth - 2013 - The algorithmic foundations of differential privacy(2).pdf:pdf},
issn = {15513068},
journal = {Foundations and Trends in Theoretical Computer Science},
number = {3-4},
pages = {211--487},
title = {{The algorithmic foundations of differential privacy}},
volume = {9},
year = {2013}
}


@article{Dong2019,
abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
doi = {10.1145/3292500.3332296},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Rekatsinas - 2019 - Data integration and machine learning A natural synergy.pdf:pdf},
isbn = {9781450362016},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {2018,Data integration, Machine learning,acm reference format,data integration,data integration and,machine learning,theodoros rekatsinas,xin luna dong and},
pages = {3193--3194},
title = {{Data integration and machine learning: A natural synergy}},
year = {2019}
}

@misc{Elbir2020,
  doi = {10.48550/ARXIV.2006.01412},
  
  url = {https://arxiv.org/abs/2006.01412},
  
  author = {Elbir, Ahmet M. and Soner, Burak and Coleri, Sinem},
  
  keywords = {Signal Processing (eess.SP), Information Theory (cs.IT), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Federated Learning in Vehicular Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Shubyn2022,
author="Shubyn, Bohdan
and Mrozek, Dariusz
and Maksymyuk, Taras
and Sunderam, Vaidy
and Kostrzewa, Daniel
and Grzesik, Piotr
and Benecki, Pawe{\l}",
editor="Groen, Derek
and de Mulatier, Cl{\'e}lia
and Paszynski, Maciej
and Krzhizhanovskaya, Valeria V.
and Dongarra, Jack J.
and Sloot, Peter M. A.",
title="Federated Learning for Anomaly Detection in Industrial IoT-enabled Production Environment Supported by Autonomous Guided Vehicles",
booktitle="Computational Science -- ICCS 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="409--421",
abstract="Intelligent production requires maximum downtime avoidance since downtimes lead to economic loss. Thus, Industry 4.0 (today's IoT-driven industrial revolution) is aimed at automated production with real-time decision-making and maximal uptime. To achieve this, new technologies such as Machine Learning (ML), Artificial Intelligence (AI), and Autonomous Guided Vehicles (AGVs) are integrated into production to optimize and automate many production processes. The increasing use of AGVs in production has far-reaching consequences for industrial communication systems. To make AGVs in production even more effective, we propose to use Federated Learning (FL) which provides a secure exchange of experience between intelligent manufacturing devices to improve prediction accuracy. We conducted research in which we exchanged experiences between the three virtual devices, and the results confirm the effectiveness of this approach in production environments.",
isbn="978-3-031-08760-8"
}

@article{Prayitno2021,
abstract = {Recent advances in deep learning have shown many successful stories in smart healthcare applications with data‐driven insight into improving clinical institutions' quality of care. Excellent deep learning models are heavily data‐driven. The more data trained, the more robust and more generalizable the performance of the deep learning model. However, pooling the medical data into centralized storage to train a robust deep learning model faces privacy, ownership, and strict regulation challenges. Federated learning resolves the previous challenges with a shared global deep learning model using a central aggregator server. At the same time, patient data remain with the local party, maintaining data anonymity and security. In this study, first, we provide a comprehensive, up‐to‐date review of research employing federated learning in healthcare applications. Second, we evaluate a set of recent challenges from a data‐centric perspective in federated learning, such as data partitioning characteristics, data distributions, data protection mechanisms, and benchmark datasets. Finally, we point out several potential challenges and future research directions in healthcare applications.},
author = {Prayitno and Shyu, Chi Ren and Putra, Karisma Trinanda and Chen, Hsing Chung and Tsai, Yuan Yu and {Tozammel Hossain}, K. S.M. and Jiang, Wei and Shae, Zon Yin},
doi = {10.3390/app112311191},
file = {:home/eferos93/Downloads/applsci-11-11191-v2.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Artificial intelligence,Data privacy‐preserving,Deep learning,Federated learning,Healthcare},
number = {23},
title = {{A systematic review of federated learning in the healthcare area: From the perspective of data properties and applications}},
volume = {11},
year = {2021}
}


@article{Yan2018,
abstract = {Item-based collaborative filtering (i.e., ICF) technique has been widely recruited to make service recommendations in the big data environment. However, the ICF technique only performs well when the data for service recommendation decision-making are stored in a physically centralized manner, while they often fail to recommend appropriate services to a target user in the distributed environment where the involved multiple parties are reluctant to release their data to each other due to privacy concerns. Considering this drawback, we improve the traditional ICF approach by integrating the locality-sensitive hashing (LSH) technique, to realize secure and reliable data publishing. Furthermore, through integrating the published data with little privacy across different platforms, appropriate services are recommended based on our suggested recommendation approach named ICFLSH. At last, simulated experiments are conducted on WS-DREAM data set. Experiment results show that ICFLSH performs better than the competitive approaches in terms of service recommendation accuracy, efficiency, and the capability of privacy-preservation.},
author = {Yan, Chao and Cui, Xinchun and Qi, Lianyong and Xu, Xiaolong and Zhang, Xuyun},
doi = {10.1109/ACCESS.2018.2863050},
file = {:home/eferos93/Downloads/Privacy-Aware_Data_Publishing_and_Integration_for_Collaborative_Service_Recommendation.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Item-based collaborative filtering,data publishing and integration,locality-sensitive hashing,privacy-preservation,service recommendation},
pages = {43021--43028},
publisher = {IEEE},
title = {{Privacy-aware data publishing and integration for collaborative service recommendation}},
volume = {6},
year = {2018}
}


@article{Khurram2020,
abstract = {The prevalence of various (and increasingly large) datasets presents the challenging problem of discovering common entities dispersed across disparate datasets. Solutions to the private record linkage problem (PRL) aim to enable such explorations of datasets in a secure manner. A two-party PRL protocol allows two parties to determine for which entities they each possess a record (either an exact matching record or a fuzzy matching record) in their respective datasets - without revealing to one another information about any entities for which they do not both possess records. Although several solutions have been proposed to solve the PRL problem, no current solution offers a fully cryptographic security guarantee while maintaining both high accuracy of output and subquadratic runtime efficiency. To this end, we propose the first known efficient PRL protocol that runs in subquadratic time, provides high accuracy, and guarantees cryptographic security in the semi-honest security model.},
author = {Khurram, Basit and Kerschbaum, Florian},
doi = {10.1109/ICDE48307.2020.00031},
file = {:home/eferos93/Downloads/SFour_A_Protocol_for_Cryptographically_Secure_Record_Linkage_at_Scale.pdf:pdf},
isbn = {9781728129037},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
keywords = {Deduplication,Entity Matching,Private Permutation,Private Record Linkage,Secure Multiparty Computation},
pages = {277--288},
title = {{SFour: A protocol for cryptographically secure record linkage at scale}},
volume = {2020-April},
year = {2020}
}


@article{Vatsalan2017,
abstract = {The growth ofBig Data, especially personal data dispersed inmultiple data sources, presents enormous opportunities and insights for businesses to explore and leverage the value of linked and integrated data. However, privacy concerns impede sharing or exchanging data for linkage across different organizations. Privacypreserving record linkage (PPRL) aims to address this problem by identifying and linking records that correspond to the same real-world entity across several data sources held by different parties without revealing any sensitive information about these entities. PPRL is increasingly being required in many real-world application areas. Examples range from public health surveillance to crime and fraud detection, and national security. PPRL for Big Data poses several challenges, with the three major ones being (1) scalability to multiple large databases, due to their massive volume and the flow of data within Big Data applications, (2) achieving high quality results of the linkage in the presence of variety and veracity of Big Data, and (3) preserving privacy and confidentiality of the entities represented in Big Data collections. In this chapter, we describe the challenges of PPRL in the context of Big Data, survey existing techniques for PPRL, and provide directions for future research.},
author = {Vatsalan, Dinusha and Sehili, Ziad and Christen, Peter and Rahm, Erhard},
doi = {10.1007/978-3-319-49340-4_25},
file = {:home/eferos93/Downloads/01_Vatsalan_Privacy-Preserving_Record_2017.pdf:pdf},
isbn = {9783319493404},
journal = {Handbook of Big Data Technologies},
keywords = {Big data,Privacy,Record linkage,Scalability},
pages = {851--895},
title = {{Privacy-preserving record linkage for big data: Current approaches and research challenges}},
year = {2017}
}

@article{Clifton,
author = {Clifton, Chris and Schadow, Gunther},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clifton, Schadow - Unknown - Privacy Preserving Data Integration and Sharing.pdf:pdf},
pages = {1--10},
title = {{Privacy Preserving Data Integration and Sharing}}
}

@article{Kuzu2013,
abstract = {The integration of information dispersed among multiple repositories is a crucial step for accurate data analysis in various domains. In support of this goal, it is critical to devise procedures for identifying similar records across distinct data sources. At the same time, to adhere to privacy regulations and policies, such procedures should protect the confidentiality of the individuals to whom the information corresponds. Various private record linkage (PRL) protocols have been proposed to achieve this goal, involving secure multi-party computation (SMC) and similarity preserving data transformation techniques. SMC methods provide secure and accurate solutions to the PRL problem, but are prohibitively expensive in practice, mainly due to excessive computational requirements. Data transformation techniques offer more practical solutions, but incur the cost of information leakage and false matches. In this paper, we introduce a novel model for practical PRL, which 1) affords controlled and limited information leakage, 2) avoids false matches resulting from data transformation. Initially, we partition the data sources into blocks to eliminate comparisons for records that are unlikely to match. Then, to identify matches, we apply an efficient SMC technique between the candidate record pairs. To enable efficiency and privacy, our model leaks a controlled amount of obfuscated data prior to the secure computations. Applied obfuscation relies on differential privacy which provides strong privacy guarantees against adversaries with arbitrary background knowledge. In addition, we illustrate the practical nature of our approach through an empirical analysis with data derived from public voter records. {\textcopyright} 2013 ACM.},
author = {Kuzu, Mehmet and Kantarcioglu, Murat and Inan, Ali and Bertino, Elisa and Durham, Elizabeth and Malin, Bradley},
doi = {10.1145/2452376.2452398},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuzu et al. - 2013 - Efficient privacy-aware record integration.pdf:pdf},
isbn = {9781450315975},
journal = {ACM International Conference Proceeding Series},
keywords = {differential privacy,privacy,record linkage,security},
pages = {167--178},
title = {{Efficient privacy-aware record integration}},
year = {2013}
}


@article{Gkoulalas-Divanis2021,
abstract = {Record linkage is the challenging task of deciding which records, coming from disparate data sources, refer to the same entity. Established back in 1946 by Halbert L. Dunn, the area of record linkage has received tremendous attention over the years due to its numerous real-world applications, and has led to a plethora of technologies, methods, metrics, and systems. A major direction in record linkage regards methods for linking records in a privacy-preserving manner, where sensitive and personally identifiable information in the records is not leaked as part of the linkage process. In this article, we provide an overview of the large body of research literature in privacy-preserving record linkage, discuss the different generations of techniques that have been proposed, their advantages and limitations, and present a taxonomy as well as an extensive survey on the latest generation of methods. We conclude this work with a roadmap to the new generation of analytics-driven techniques that aims to address some of the major challenges in the field.},
author = {Gkoulalas-Divanis, Aris and Vatsalan, Dinusha and Karapiperis, Dimitrios and Kantarcioglu, Murat},
doi = {10.1109/TIFS.2021.3114026},
file = {:home/eferos93/Downloads/Modern_Privacy-Preserving_Record_Linkage_Techniques_An_Overview.pdf:pdf},
issn = {15566021},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {Databases,data privacy,information sharing},
pages = {4966--4987},
publisher = {IEEE},
title = {{Modern Privacy-Preserving Record Linkage Techniques: An Overview}},
volume = {16},
year = {2021}
}


@article{Nobrega2021,
abstract = {Privacy-Preserving Record Linkage (PPRL) intends to integrate private data from several data sources held by different parties. Due to recent laws and regulations (e.g, General Data Protection Regulation), PPRL approaches are increasingly demanded in real-world application areas such as health-care, credit analysis, public policy evaluation, and national security. However, the majority of the PPRL approaches consider an unrealistic adversary model, particularly the Honest but Curious (HBC) model, which assumes that all PPRL parties will follow a pre-agreed data integration protocol, and will not try to break the confidentiality of the data handled during the process. The HBC model is hard to employ in real-world applications, mainly because of the need to trust other parties fully. To overcome the limitations associated with the majority of the adversary models considered by PPRL approaches, we propose a protocol that considers covert adversaries, i.e., adversaries that may deviate arbitrarily from the protocol specification in an attempt to cheat. In such protocol, however, the honest parties are able to detect this misbehavior with a high probability. To provide a proof-of-concept implementation of this protocol, we employ the Blockchain technology and propose an improvement in the most used anonymization technique for PPRL, the Bloom Filter. The evaluation carried out using several real-world data sources has demonstrated the effectiveness (linkage quality) obtained by our contributions, as well as the ability to detect the misbehavior of a malicious adversary during the PPRL execution.},
author = {N{\'{o}}brega, Thiago and Pires, Carlos Eduardo S. and Nascimento, Dimas Cassimiro},
doi = {10.1016/j.is.2021.101826},
file = {:home/eferos93/Downloads/1-s2.0-S0306437921000661-main.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Blockchain,Bloom Filter,Data privacy,Entity resolution,Privacy preserving entity resolution},
pages = {101826},
publisher = {Elsevier Ltd.},
title = {{Blockchain-based Privacy-Preserving Record Linkage: enhancing data privacy in an untrusted environment}},
url = {https://doi.org/10.1016/j.is.2021.101826},
volume = {102},
year = {2021}
}
