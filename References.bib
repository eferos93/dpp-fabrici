@article{recordLinkTheory,
author = { Ivan P.   Fellegi  and  Alan B.   Sunter },
title = {A Theory for Record Linkage},
journal = {Journal of the American Statistical Association},
volume = {64},
number = {328},
pages = {1183-1210},
year  = {1969},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1969.10501049},
}

@inproceedings{dataModelIntegration1980,
author = {El-Masri, Ramez and Wiederhold, Gio},
title = {Data Model Integration Using the Structural Model},
year = {1979},
isbn = {089791001X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/582095.582127},
doi = {10.1145/582095.582127},
abstract = {One approach to the design of a logical model for an integrated database requires each potential user or application to specify its view as a data model. An integration phase follows, where these user data models are integrated into a global database model. We address the problem of view integration when user data models are expressed using the structural model [Wi77, WE79].The structural model is built from relations in Boyce-Codd normal form [Co74]. A basic set of integrity assertions is implicit in the model. The integrity assertions are defined by classification of relations into types, and are represented by connections between relations. We will show how to integrate different representations of two related real-world entity classes.},
booktitle = {Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data},
pages = {191â€“202},
numpages = {12},
keywords = {data semantics, data model integration, logical database design, conceptual and external schema, structural model, entity classes and relationships, ansi/sparc dbms architecture, relational model},
location = {Boston, Massachusetts},
series = {SIGMOD '79}
}

@article{Dong2013,
abstract = {The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community. {\textcopyright} 2013 IEEE.},
author = {Dong, Xin Luna and Srivastava, Divesh},
doi = {10.1109/ICDE.2013.6544914},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Srivastava - 2013 - Big data integration.pdf:pdf},
isbn = {9781467349086},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
keywords = {Publisher: Morgan & Claypool Publishers, Composito},
number = {September},
pages = {1245--1248},
title = {{Big data integration}},
year = {2013}
}

@article{yu2016big,
  title={Big privacy: Challenges and opportunities of privacy study in the age of big data},
  author={Yu, Shui},
  journal={IEEE access},
  volume={4},
  pages={2751--2763},
  year={2016},
  publisher={IEEE}
}

@article{Cormode2011,
abstract = {Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance.We also discuss the tradeoffs between the different synopsis types. {\textcopyright} 2012 G. Cormode, M. Garofalakis, P. J. Haas and C. Jermaine.},
author = {Cormode, Graham and Garofalakis, Minos and Haas, Peter J. and Jermaine, Chris},
doi = {10.1561/1900000004},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cormode et al. - 2011 - Synopses for massive data Samples, histograms, wavelets, sketches.pdf:pdf},
issn = {19317883},
journal = {Foundations and Trends in Databases},
number = {1-3},
pages = {1--294},
title = {{Synopses for massive data: Samples, histograms, wavelets, sketches}},
volume = {4},
year = {2011}
}



