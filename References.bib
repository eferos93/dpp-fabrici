@article{recordLinkTheory,
author = { Ivan P.   Fellegi  and  Alan B.   Sunter },
title = {A Theory for Record Linkage},
journal = {Journal of the American Statistical Association},
volume = {64},
number = {328},
pages = {1183-1210},
year  = {1969},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1969.10501049},
}

@inproceedings{dataModelIntegration1980,
author = {El-Masri, Ramez and Wiederhold, Gio},
title = {Data Model Integration Using the Structural Model},
year = {1979},
isbn = {089791001X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/582095.582127},
doi = {10.1145/582095.582127},
abstract = {One approach to the design of a logical model for an integrated database requires each potential user or application to specify its view as a data model. An integration phase follows, where these user data models are integrated into a global database model. We address the problem of view integration when user data models are expressed using the structural model [Wi77, WE79].The structural model is built from relations in Boyce-Codd normal form [Co74]. A basic set of integrity assertions is implicit in the model. The integrity assertions are defined by classification of relations into types, and are represented by connections between relations. We will show how to integrate different representations of two related real-world entity classes.},
booktitle = {Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data},
pages = {191–202},
numpages = {12},
keywords = {data semantics, data model integration, logical database design, conceptual and external schema, structural model, entity classes and relationships, ansi/sparc dbms architecture, relational model},
location = {Boston, Massachusetts},
series = {SIGMOD '79}
}

@article{Dong2013,
abstract = {The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community. {\textcopyright} 2013 IEEE.},
author = {Dong, Xin Luna and Srivastava, Divesh},
doi = {10.1109/ICDE.2013.6544914},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Srivastava - 2013 - Big data integration.pdf:pdf},
isbn = {9781467349086},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
keywords = {Publisher: Morgan & Claypool Publishers, Composito},
number = {September},
pages = {1245--1248},
title = {{Big data integration}},
year = {2013}
}

@article{yu2016big,
  title={Big privacy: Challenges and opportunities of privacy study in the age of big data},
  author={Yu, Shui},
  journal={IEEE access},
  volume={4},
  pages={2751--2763},
  year={2016},
  publisher={IEEE}
}

@article{Cormode2011,
abstract = {Methods for Approximate Query Processing (AQP) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in AQP. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time efficiency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance.We also discuss the tradeoffs between the different synopsis types. {\textcopyright} 2012 G. Cormode, M. Garofalakis, P. J. Haas and C. Jermaine.},
author = {Cormode, Graham and Garofalakis, Minos and Haas, Peter J. and Jermaine, Chris},
doi = {10.1561/1900000004},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cormode et al. - 2011 - Synopses for massive data Samples, histograms, wavelets, sketches.pdf:pdf},
issn = {19317883},
journal = {Foundations and Trends in Databases},
number = {1-3},
pages = {1--294},
title = {{Synopses for massive data: Samples, histograms, wavelets, sketches}},
volume = {4},
year = {2011}
}


@article{Gribonval2020,
abstract = {This article considers "compressive learning," an approach to large-scale machine learning where datasets are massively compressed before learning (e.g., clustering, classification, or regression) is performed. In particular, a "sketch" is first constructed by computing carefully chosen nonlinear random features (e.g., random Fourier features) and averaging them over the whole dataset. Parameters are then learned from the sketch, without access to the original dataset. This article surveys the current state-of-the-art in compressive learning, including the main concepts and algorithms, their connections with established signal-processing methods, existing theoretical guarantees -- on both information preservation and privacy preservation, and important open problems.},
archivePrefix = {arXiv},
arxivId = {2008.01839},
author = {Gribonval, R{\'{e}}mi and Chatalic, Antoine and Keriven, Nicolas and Schellekens, Vincent and Jacques, Laurent and Schniter, Philip},
eprint = {2008.01839},
file = {:home/eferos93/Downloads/2008.01839.pdf:pdf},
pages = {1--35},
title = {{Sketching Datasets for Large-Scale Learning (long version)}},
url = {http://arxiv.org/abs/2008.01839},
year = {2020}
}

@article{Antonanzas2021,
abstract = {Time series data can be subject to changes in the underlying process that generates them and, because of these changes, models built on old samples can become obsolete or perform poorly. In this work, we present a way to incorporate information about the current data distribution and its evolution across time into machine learning algorithms. Our solution is based on efficiently maintaining statistics, particularly the mean and the variance, of data features at different time resolutions. These data summarisations can be performed over the input attributes, in which case they can then be fed into the model as additional input features, or over latent representations learned by models, such as those of Recurrent Neural Networks. In classification tasks, the proposed techniques can significantly outperform the prediction capabilities of equivalent architectures with no feature / latent summarisations. Furthermore, these modifications do not introduce notable computational and memory overhead when properly adjusted.},
archivePrefix = {arXiv},
arxivId = {2108.11923},
author = {Antonanzas, Jesus and Arias, Marta and Bifet, Albert},
eprint = {2108.11923},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antonanzas, Arias, Bifet - 2021 - Sketches for Time-Dependent Machine Learning(2).pdf:pdf},
keywords = {machine learning,recurrent neural networks,stream processing,time series analysis},
number = {1},
pages = {1--9},
title = {{Sketches for Time-Dependent Machine Learning}},
url = {http://arxiv.org/abs/2108.11923},
year = {2021}
}


@article{Jiang2018,
abstract = {To address the challenge of explosive big data, distributed machine learning (ML) has drawn the interests of many researchers. Since many distributed ML algorithms trained by stochastic gradient descent (SGD) involve communicating gradients through the network, it is important to compress the transferred gradient. A category of low-precision algorithms can significantly reduce the size of gradients, at the expense of some precision loss. However, existing low-precision methods are not suitable for many cases where the gradients are sparse and nonuniformly distributed. In this paper, we study is there a compression method that can efficiently handle a sparse and nonuniform gradient consisting of key-value pairs? Our first contribution is a sketch based method that compresses the gradient values. Sketch is a class of algorithms using a probabilistic data structure to approximate the distribution of input data. We design a quantile-bucket quantification method that uses a quantile sketch to sort gradient values into buckets and encodes them with the bucket indexes. To further compress the bucket indexes, our second contribution is a sketch algorithm, namely MinMaxSketch. MinMaxSketch builds a set of hash tables and solves hash collisions with a MinMax strategy. The third contribution of this paper is a delta-binary encoding method that calculates the increment of the gradient keys and stores them with fewer bytes. We also theoretically discuss the correctness and the error bound of three proposed methods. To the best of our knowledge, this is the first effort combining data sketch with ML. We implement a prototype system in a real cluster of our industrial partner Tencent Inc., and show that our method is up to 10× faster than existing methods.},
author = {Jiang, Jiawei and Fu, Fangcheng and Yang, Tong and Cui, Bin},
doi = {10.1145/3183713.3196894},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2018 - SketchML Accelerating distributed machine learning with data sketches.pdf:pdf},
isbn = {9781450317436},
issn = {07308078},
journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
keywords = {Distributed machine learning,Frequency sketch,Quantification,Quantile sketch,Stochastic gradient Descent},
pages = {1269--1284},
title = {{SketchML: Accelerating distributed machine learning with data sketches}},
year = {2018}
}

@article{Dwork2013,
abstract = {The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition. After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations - not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed. We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed. Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey- there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it. {\textcopyright} 2014 C. Dwork and A. Roth.},
author = {Dwork, Cynthia and Roth, Aaron},
doi = {10.1561/0400000042},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dwork, Roth - 2013 - The algorithmic foundations of differential privacy(2).pdf:pdf},
issn = {15513068},
journal = {Foundations and Trends in Theoretical Computer Science},
number = {3-4},
pages = {211--487},
title = {{The algorithmic foundations of differential privacy}},
volume = {9},
year = {2013}
}


@article{Dong2019,
abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
doi = {10.1145/3292500.3332296},
file = {:home/eferos93/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Rekatsinas - 2019 - Data integration and machine learning A natural synergy.pdf:pdf},
isbn = {9781450362016},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {2018,Data integration, Machine learning,acm reference format,data integration,data integration and,machine learning,theodoros rekatsinas,xin luna dong and},
pages = {3193--3194},
title = {{Data integration and machine learning: A natural synergy}},
year = {2019}
}


